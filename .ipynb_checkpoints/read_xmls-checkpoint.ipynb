{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from unidecode import unidecode\n",
    "import altair as alt\n",
    "tqdm.pandas()\n",
    "from collections import Counter\n",
    "import string\n",
    "from genderize import Genderize\n",
    "# genderize = Genderize(\n",
    "#     user_agent='GenderizeDocs/0.0',\n",
    "#     api_key='c363eacf807f4af4992b358200ebc15c',\n",
    "#     timeout=30.0)\n",
    "with open('dict_genders.pickle', 'rb') as handle:\n",
    "    dict_genders = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_authors(dc):\n",
    "    if type(dc) == list:\n",
    "        result = [\n",
    "            unidecode(i.get(\"#text\").title().strip())\n",
    "            for i in dc\n",
    "            if i.get(\"#text\") is not None and i.get(\"@scheme\") != \"institution\"\n",
    "        ]\n",
    "        return [x for x in result if x != \"And Others\"]\n",
    "    elif dc.get(\"#text\") is not None and dc.get(\"@scheme\") != \"institution\":\n",
    "        return [unidecode(dc.get(\"#text\").title().strip())]\n",
    "\n",
    "\n",
    "def clean_name(name):\n",
    "    if ', ' in name:\n",
    "        lst = name.split(', ')\n",
    "        lst = [item.split(' ')[0] for item in lst]\n",
    "        return lst[1] + ' ' + lst[0]\n",
    "    elif ',' in name:\n",
    "        lst = name.split(',')\n",
    "        lst = [item.split(' ')[0] for item in lst]\n",
    "        return lst[1] + ' ' + lst[0]\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "def get_first_names(author_list):\n",
    "\n",
    "    if len(author_list) > 0:\n",
    "        names = []\n",
    "        for x in author_list:\n",
    "            words_in_name = len(x.split())\n",
    "\n",
    "            if words_in_name > 0:\n",
    "                first = re.sub(r'[^\\w\\s]','', x.split()[0])\n",
    "            if words_in_name > 1:\n",
    "                second = re.sub(r'[^\\w\\s]','', x.split()[1])\n",
    "            if words_in_name > 2:\n",
    "                third = re.sub(r'[^\\w\\s]','', x.split()[2])\n",
    "            if words_in_name > 3:\n",
    "                fourth = re.sub(r'[^\\w\\s]','', x.split()[3])\n",
    "\n",
    "            if  words_in_name > 0 and len(first) > 1:\n",
    "                names.append(first)\n",
    "            elif words_in_name > 1 and len(second) > 1:\n",
    "                names.append(second)\n",
    "            elif words_in_name > 2 and len(third) > 1:\n",
    "                names.append(third)\n",
    "            elif words_in_name > 3 and len(fourth) > 1:\n",
    "                names.append(fourth)\n",
    "            else:\n",
    "                names.append(x)\n",
    "\n",
    "        return names\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def name_to_gender(first_name_list):\n",
    "    if first_name_list and len(first_name_list) > 0:\n",
    "        return [dict_genders[name] for name in first_name_list]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_edges(auth_list):\n",
    "    return list(combinations(auth_list, 2))\n",
    "\n",
    "def extract_ids(dc):\n",
    "    if type(dc) == list:\n",
    "        return [\n",
    "            i.get(\"#text\").upper().strip()\n",
    "            for i in dc\n",
    "            if i.get(\"#text\") is not None and i.get(\"@scheme\") == \"eric_accno\"\n",
    "        ][0]\n",
    "    elif dc.get(\"#text\") is not None and dc.get(\"@scheme\") == \"eric_accno\":\n",
    "        return dc.get(\"#text\").upper().strip()\n",
    "    \n",
    "def extract_subject(dc):\n",
    "    result = []\n",
    "    for item in dc:\n",
    "        if type(item) == collections.OrderedDict:\n",
    "            result.append(unidecode(item.get(\"#text\").title().strip()))\n",
    "        elif type(item) == str:\n",
    "            result.append(unidecode(item.title().strip()))\n",
    "        else:\n",
    "            result.append(unidecode(item.title().strip()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = []\n",
    "\n",
    "for year in tqdm(range(1965, 2021)):\n",
    "    file_name = \"data/eric\" + str(year)\n",
    "    with open(file_name + \".xml\", encoding=\"utf-8\") as fd:\n",
    "        dict = xmltodict.parse(fd.read())\n",
    "    recs = [rec[\"metadata\"] for rec in dict[\"records\"][\"record\"]]\n",
    "    df = pd.DataFrame(recs)\n",
    "    df = df[df['dc:type'].notna()]\n",
    "    df = df[df['eric:peer_reviewed'].notna()]\n",
    "    df['type'] = [''.join(map(str, l)).lower() for l in df['dc:type']]\n",
    "    df = df.loc[df['eric:peer_reviewed'] == 'T']\n",
    "    # df = df[['ids', 'authors', 'edges', 'dc:type', 'dc:subject', 'eric:keywords', 'eric:keywords_geo', 'dc:title', 'eric:pageCount', 'dc:date', 'eric:dateAdded']]\n",
    "    df_all.append(df)\n",
    "\n",
    "df_all = pd.concat(df_all)\n",
    "\n",
    "df_all = df_all.loc[(df_all['type'].str.contains(\"journal\"))]\n",
    "df_all[\"authors\"] = df_all.apply(lambda row: extract_authors(row[\"dc:creator\"]), axis=1)\n",
    "df_all = df_all[df_all['authors'].notna()] # remove nans\n",
    "df_all['authors'] = df_all.apply(lambda row: [clean_name(item) for item in row['authors']], axis=1) # clean names\n",
    "df_all[\"author_first_names\"] = df_all.apply(lambda row: get_first_names(row[\"authors\"]), axis=1) # get first names for genderize\n",
    "df_all[\"author_genders\"] = df_all.apply(lambda row: name_to_gender(row[\"author_first_names\"]), axis=1) # get genders from dict\n",
    "df_all['n_authors'] = df_all.apply(lambda row: len(row[\"authors\"]), axis=1)\n",
    "df_all[\"edges\"] = df_all.apply(lambda row: get_edges(sorted(row[\"authors\"])), axis=1) # get edges\n",
    "df_all[\"ids\"] = df_all.apply(lambda row: extract_ids(row[\"dc:identifier\"]), axis=1)\n",
    "\n",
    "df_all = df_all[df_all['dc:subject'].notna()]\n",
    "df_all[\"subjects\"] = df_all.apply(lambda row: extract_subject(row[\"dc:subject\"]), axis=1)\n",
    "df_all.loc[:, 'subject_top'] = df_all.subjects.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE the API ##\n",
    "# node_list1 = df_all[\"author_first_names\"].tolist()\n",
    "# node_list2 = [x for x in node_list1 if x is not None]  # remove none\n",
    "# node_list3 = [item for sublist in node_list2 for item in sublist]\n",
    "# node_list = list(set(node_list3))\n",
    "\n",
    "# df_genders = pd.DataFrame(genderize.get(node_list))\n",
    "# df_genders.to_csv(\"df_genders.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "# dict_genders = pd.Series(df_genders.gender.values,index=df_genders.name).to_dict()\n",
    "# with open('dict_genders.pickle', 'wb') as handle:\n",
    "#     pickle.dump(dict_genders, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_order = pd.DataFrame(df_all[df_all['author_genders'].notna()].author_genders.tolist(), index= df_all[df_all['author_genders'].notna()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_order[0].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_order[1].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_order[2].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_order[3].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_order[4].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_names2(author_list):\n",
    "\n",
    "    if len(author_list) > 0:\n",
    "        names = []\n",
    "        for x in author_list:\n",
    "            words_in_name = len(x.split())\n",
    "\n",
    "            if words_in_name > 0:\n",
    "                first = re.sub(r'[^\\w\\s]','', x.split()[0])\n",
    "            if words_in_name > 1:\n",
    "                second = re.sub(r'[^\\w\\s]','', x.split()[1])\n",
    "            if words_in_name > 2:\n",
    "                third = re.sub(r'[^\\w\\s]','', x.split()[2])\n",
    "            if words_in_name > 3:\n",
    "                fourth = re.sub(r'[^\\w\\s]','', x.split()[3])\n",
    "\n",
    "            if  words_in_name > 0 and len(first) > 1:\n",
    "                names.append((first, dict_genders[first]))\n",
    "            elif words_in_name > 1 and len(second) > 1:\n",
    "                names.append((second, dict_genders[second]))\n",
    "            elif words_in_name > 2 and len(third) > 1:\n",
    "                names.append((third, dict_genders[third]))\n",
    "            elif words_in_name > 3 and len(fourth) > 1:\n",
    "                names.append((fourth, dict_genders[fourth]))\n",
    "            else:\n",
    "                names.append((x, dict_genders[x]))\n",
    "\n",
    "        return names\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list1 = df_all[\"authors\"].tolist()\n",
    "node_list2 = [x for x in node_list1 if x is not None]  # remove none\n",
    "node_list3 = [item for sublist in node_list2 for item in sublist]\n",
    "node_list = list(set(node_list3))\n",
    "len(node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors = pd.DataFrame(list(zip(node_list, get_first_names2(node_list))), columns = [\"full_name\", \"tuple\"])\n",
    "df_authors[['first_name','gender']] = pd.DataFrame(df_authors.tuple.tolist(), index= df_authors.index)\n",
    "df_authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female    0.501827\n",
       "male      0.464286\n",
       "NaN       0.033887\n",
       "Name: gender, dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_authors[\"gender\"].value_counts(normalize = True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.groupby(['eric:dateAdded'])['eric:issn'].apply(lambda x: x.isnull().mean())\n",
    "# df_all[\"dc:subject\"].iloc[328615]\n",
    "#df_all[['subjects', 'subject_top']].to_csv(\"subjects.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['eric:dateAdded'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(df_local):\n",
    "\n",
    "    node_list1 = df_local[\"authors\"].tolist()\n",
    "    node_list2 = [x for x in node_list1 if x is not None]  # remove none\n",
    "    node_list3 = [item for sublist in node_list2 for item in sublist]\n",
    "    node_list = list(set(node_list3))\n",
    "\n",
    "    n_papers_per_author = len(node_list3)/len(node_list)\n",
    "    \n",
    "    edge_list1 = df_local[\"edges\"].tolist()\n",
    "    edge_list2 = [x for x in edge_list1 if x is not None]  # remove none\n",
    "    edge_list = [item for sublist in edge_list2 for item in sublist]\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(node_list)\n",
    "    G.add_edges_from(edge_list)\n",
    "    return (G, n_papers_per_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year by year (NOT cumulative)\n",
    "list1 = []\n",
    "\n",
    "for year in tqdm(range(1965, 2021)):\n",
    "    \n",
    "    df_local = df_all.loc[df_all['eric:dateAdded'] == str(year)]\n",
    "    \n",
    "    if len(df_local) == 0:\n",
    "        continue\n",
    "    \n",
    "    result = generate_graph(df_local)\n",
    "    G = result[0]\n",
    "    n_authors = len(G)\n",
    "    n_papers = len(df_local)\n",
    "    \n",
    "    n_authors_per_paper = df_local['n_authors'].mean()\n",
    "    n_papers_per_author = result[1]\n",
    "    \n",
    "    n_collabs = nx.number_of_edges(G)\n",
    "    n_isolates = nx.number_of_isolates(G)\n",
    "    mean_collabs = 2 * G.number_of_edges() / float(G.number_of_nodes())\n",
    "    \n",
    "    G_largest_comp = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\n",
    "    largest_component = len(G_largest_comp)/len(G) \n",
    "    \n",
    "    deg_assort = nx.degree_assortativity_coefficient(G)\n",
    "    avg_clustering = nx.average_clustering(G)\n",
    "    transitivity = nx.transitivity(G)\n",
    "    \n",
    "    list1.append((year, n_authors, n_papers, n_authors_per_paper, n_papers_per_author, n_collabs, n_isolates, mean_collabs, largest_component, deg_assort, avg_clustering, transitivity))\n",
    "\n",
    "df_summary = pd.DataFrame(list1, columns = [\"year\", \"n_authors\", \"n_papers\", \"n_authors_per_paper\", \"n_papers_per_author\", \"n_collabs\", \"n_isolates\", \"mean_collabs\", \"largest_component\", \"deg_assort\", \"avg_clustering\", \"transitivity\"])\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\",\n",
    "    interpolate='step-after',\n",
    "    line=True\n",
    ").encode(\n",
    "    x='year',\n",
    "    y='n_authors'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\",\n",
    "    interpolate='step-after',\n",
    "    line=True\n",
    ").encode(\n",
    "    x='year',\n",
    "    y='n_papers'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\",\n",
    "    interpolate='step-after',\n",
    "    line=True\n",
    ").encode(\n",
    "    x='year',\n",
    "    y='n_authors_per_paper'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\",\n",
    "    interpolate='step-after',\n",
    "    line=True\n",
    ").encode(\n",
    "    x='year',\n",
    "    y='n_papers_per_author'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\",\n",
    "    interpolate='step-after',\n",
    "    line=True\n",
    ").encode(\n",
    "    x='year',\n",
    "    y='n_collabs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\",\n",
    "    interpolate='step-after',\n",
    "    line=True\n",
    ").encode(\n",
    "    x='year',\n",
    "    y='mean_collabs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\",\n",
    "    interpolate='step-after',\n",
    "    line=True\n",
    ").encode(\n",
    "    x='year',\n",
    "    y='n_isolates'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\",\n",
    "    interpolate='step-after',\n",
    "    line=True\n",
    ").encode(\n",
    "    x='year',\n",
    "    y='largest_component'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\",\n",
    "    interpolate='step-after',\n",
    "    line=True\n",
    ").encode(\n",
    "    x='year',\n",
    "    y='deg_assort'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\",\n",
    "    interpolate='step-after',\n",
    "    line=True\n",
    ").encode(\n",
    "    x='year',\n",
    "    y='avg_clustering'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\",\n",
    "    interpolate='step-after',\n",
    "    line=True\n",
    ").encode(\n",
    "    x='year',\n",
    "    y='transitivity'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "\n",
    "result = generate_graph(df_all)\n",
    "G = result[0]\n",
    "n_authors = len(G)\n",
    "n_papers = len(df_all)\n",
    "\n",
    "n_authors_per_paper = df_all['n_authors'].mean()\n",
    "n_papers_per_author = result[1]\n",
    "\n",
    "n_collabs = nx.number_of_edges(G)\n",
    "n_isolates = nx.number_of_isolates(G)\n",
    "mean_collabs = 2 * G.number_of_edges() / float(G.number_of_nodes())\n",
    "\n",
    "G_largest_comp = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\n",
    "largest_component = len(G_largest_comp)/len(G) \n",
    "\n",
    "deg_assort = nx.degree_assortativity_coefficient(G)\n",
    "avg_clustering = nx.average_clustering(G)\n",
    "transitivity = nx.transitivity(G)\n",
    "\n",
    "list1.append((year, n_authors, n_papers, n_authors_per_paper, n_papers_per_author, n_collabs, n_isolates, mean_collabs, largest_component, deg_assort, avg_clustering, transitivity))\n",
    "\n",
    "df_overall = pd.DataFrame(list1, columns = [\"year\", \"n_authors\", \"n_papers\", \"n_authors_per_paper\", \"n_papers_per_author\", \"n_collabs\", \"n_isolates\", \"mean_collabs\", \"largest_component\", \"deg_assort\", \"avg_clustering\", \"transitivity\"])\n",
    "df_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.info(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "def draw_ego(name):\n",
    "    hub_ego = nx.ego_graph(G, name)\n",
    "    pos = nx.spring_layout(hub_ego)\n",
    "    nx.draw(hub_ego, pos, node_color=\"b\", node_size=50, with_labels=True)\n",
    "    options = {\"node_size\": 300, \"node_color\": \"r\"}\n",
    "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], **options)\n",
    "\n",
    "draw_ego(\"Linda Darling-Hammond\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_summary.to_csv(\"df_summary.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "\n",
    "for x in tqdm(G.nodes()):\n",
    "    words_in_name = len(x.split())\n",
    "    \n",
    "    if words_in_name > 0:\n",
    "        first = re.sub(r'[^\\w\\s]','', x.split()[0])\n",
    "    if words_in_name > 1:\n",
    "        second = re.sub(r'[^\\w\\s]','', x.split()[1])\n",
    "    if words_in_name > 2:\n",
    "        third = re.sub(r'[^\\w\\s]','', x.split()[2])\n",
    "    \n",
    "    if  words_in_name > 0 and len(first) > 1:\n",
    "        names.append(first)\n",
    "    elif words_in_name > 1 and len(second) > 1:\n",
    "        names.append(second)\n",
    "    elif words_in_name > 2 and len(third) > 1:\n",
    "        names.append(third)\n",
    "\n",
    "Counter(names).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
