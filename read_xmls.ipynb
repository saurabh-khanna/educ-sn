{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from unidecode import unidecode\n",
    "import altair as alt\n",
    "\n",
    "tqdm.pandas()\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# from genderize import Genderize\n",
    "# genderize = Genderize(\n",
    "#     user_agent='GenderizeDocs/0.0',\n",
    "#     api_key='c363eacf807f4af4992b358200ebc15c',\n",
    "#     timeout=30.0)\n",
    "with open(\"dict_genders.pickle\", \"rb\") as handle:\n",
    "    dict_genders = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_authors(dc):\n",
    "    if type(dc) == list:\n",
    "        result = [\n",
    "            unidecode(i.get(\"#text\").title().strip())\n",
    "            for i in dc\n",
    "            if i.get(\"#text\") is not None and i.get(\"@scheme\") != \"institution\"\n",
    "        ]\n",
    "        return [x for x in result if x != \"And Others\"]\n",
    "    elif dc.get(\"#text\") is not None and dc.get(\"@scheme\") != \"institution\":\n",
    "        return [unidecode(dc.get(\"#text\").title().strip())]\n",
    "\n",
    "\n",
    "def clean_name(name):\n",
    "    if \", \" in name:\n",
    "        lst = name.split(\", \")\n",
    "        lst = [item.split(\" \")[0] for item in lst]\n",
    "        return lst[1] + \" \" + lst[0]\n",
    "    elif \",\" in name:\n",
    "        lst = name.split(\",\")\n",
    "        lst = [item.split(\" \")[0] for item in lst]\n",
    "        return lst[1] + \" \" + lst[0]\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "\n",
    "def get_first_names(author_list):\n",
    "\n",
    "    if len(author_list) > 0:\n",
    "        names = []\n",
    "        for x in author_list:\n",
    "            words_in_name = len(x.split())\n",
    "\n",
    "            if words_in_name > 0:\n",
    "                first = re.sub(r\"[^\\w\\s]\", \"\", x.split()[0])\n",
    "            if words_in_name > 1:\n",
    "                second = re.sub(r\"[^\\w\\s]\", \"\", x.split()[1])\n",
    "            if words_in_name > 2:\n",
    "                third = re.sub(r\"[^\\w\\s]\", \"\", x.split()[2])\n",
    "            if words_in_name > 3:\n",
    "                fourth = re.sub(r\"[^\\w\\s]\", \"\", x.split()[3])\n",
    "\n",
    "            if words_in_name > 0 and len(first) > 1:\n",
    "                names.append(first)\n",
    "            elif words_in_name > 1 and len(second) > 1:\n",
    "                names.append(second)\n",
    "            elif words_in_name > 2 and len(third) > 1:\n",
    "                names.append(third)\n",
    "            elif words_in_name > 3 and len(fourth) > 1:\n",
    "                names.append(fourth)\n",
    "            else:\n",
    "                names.append(x)\n",
    "\n",
    "        return names\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def name_to_gender(first_name_list):\n",
    "    if first_name_list and len(first_name_list) > 0:\n",
    "        return [dict_genders[name] for name in first_name_list]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_edges(auth_list):\n",
    "    return list(combinations(auth_list, 2))\n",
    "\n",
    "\n",
    "def extract_ids(dc):\n",
    "    if type(dc) == list:\n",
    "        return [\n",
    "            i.get(\"#text\").upper().strip()\n",
    "            for i in dc\n",
    "            if i.get(\"#text\") is not None and i.get(\"@scheme\") == \"eric_accno\"\n",
    "        ][0]\n",
    "    elif dc.get(\"#text\") is not None and dc.get(\"@scheme\") == \"eric_accno\":\n",
    "        return dc.get(\"#text\").upper().strip()\n",
    "\n",
    "\n",
    "def extract_subject(dc):\n",
    "    result = []\n",
    "    for item in dc:\n",
    "        if type(item) == collections.OrderedDict:\n",
    "            result.append(unidecode(item.get(\"#text\").title().strip()))\n",
    "        elif type(item) == str:\n",
    "            result.append(unidecode(item.title().strip()))\n",
    "        else:\n",
    "            result.append(unidecode(item.title().strip()))\n",
    "    return result\n",
    "\n",
    "\n",
    "def plot_degree_dist(G):\n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    plt.hist(degrees)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 56/56 [07:25<00:00,  7.96s/it]\n"
     ]
    }
   ],
   "source": [
    "df_all = []\n",
    "\n",
    "for year in tqdm(range(1965, 2021)):\n",
    "    file_name = \"data/eric\" + str(year)\n",
    "    with open(file_name + \".xml\", encoding=\"utf-8\") as fd:\n",
    "        dict = xmltodict.parse(fd.read())\n",
    "    recs = [rec[\"metadata\"] for rec in dict[\"records\"][\"record\"]]\n",
    "    df = pd.DataFrame(recs)\n",
    "    df = df[df[\"dc:type\"].notna()]\n",
    "    df = df[df[\"eric:peer_reviewed\"].notna()]\n",
    "    df[\"type\"] = [\"\".join(map(str, l)).lower() for l in df[\"dc:type\"]]\n",
    "    df = df.loc[df[\"eric:peer_reviewed\"] == \"T\"]\n",
    "    # df = df[['ids', 'authors', 'edges', 'dc:type', 'dc:subject', 'eric:keywords', 'eric:keywords_geo', 'dc:title', 'eric:pageCount', 'dc:date', 'eric:dateAdded']]\n",
    "    df_all.append(df)\n",
    "\n",
    "df_all = pd.concat(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book/product reviewsjournal articles                                                             764\n",
       "journal articlesbook/product reviews                                                             351\n",
       "bookscollected works - general                                                                   231\n",
       "book/product reviewsjournal articlesreports - descriptive                                         66\n",
       "book/product reviewsreports - evaluativejournal articles                                          54\n",
       "                                                                                                ... \n",
       "opinion papersreports - descriptivebook/product reviewsjournal articles                            1\n",
       "book/product reviewshistorical materialsjournal articles                                           1\n",
       "book/product reviewsreference materials - bibliographiesreports - descriptivejournal articles      1\n",
       "journal articlesbooksreports - research                                                            1\n",
       "booksreports - evaluativecollected works - general                                                 1\n",
       "Name: type, Length: 173, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.loc[(df_all[\"type\"].str.contains(\"book\"))][\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.loc[(df_all[\"type\"].str.contains(\"book\"))]\n",
    "\n",
    "df_all[\"authors\"] = df_all.apply(lambda row: extract_authors(row[\"dc:creator\"]), axis=1)\n",
    "df_all = df_all[df_all[\"authors\"].notna()]  # remove nans\n",
    "df_all[\"authors\"] = df_all.apply(\n",
    "    lambda row: [clean_name(item) for item in row[\"authors\"]], axis=1\n",
    ")  # clean names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get author first name and then gender\n",
    "df_all[\"author_first_names\"] = df_all.apply(\n",
    "    lambda row: get_first_names(row[\"authors\"]), axis=1\n",
    ")\n",
    "df_all[\"author_genders\"] = df_all.apply(\n",
    "    lambda row: name_to_gender(row[\"author_first_names\"]), axis=1\n",
    ")  # get genders from dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"n_authors\"] = df_all.apply(lambda row: len(row[\"authors\"]), axis=1)\n",
    "df_all[\"edges\"] = df_all.apply(\n",
    "    lambda row: get_edges(sorted(row[\"authors\"])), axis=1\n",
    ")  # get edges\n",
    "df_all[\"ids\"] = df_all.apply(lambda row: extract_ids(row[\"dc:identifier\"]), axis=1)\n",
    "\n",
    "df_all = df_all[df_all[\"dc:subject\"].notna()]\n",
    "df_all[\"subjects\"] = df_all.apply(\n",
    "    lambda row: extract_subject(row[\"dc:subject\"]), axis=1\n",
    ")\n",
    "df_all.loc[:, \"subject_top\"] = df_all.subjects.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE the API ##\n",
    "# node_list1 = df_all[\"author_first_names\"].tolist()\n",
    "# node_list2 = [x for x in node_list1 if x is not None]  # remove none\n",
    "# node_list3 = [item for sublist in node_list2 for item in sublist]\n",
    "# node_list = list(set(node_list3))\n",
    "\n",
    "# df_genders = pd.DataFrame(genderize.get(node_list))\n",
    "# df_genders.to_csv(\"df_genders.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "# dict_genders = pd.Series(df_genders.gender.values,index=df_genders.name).to_dict()\n",
    "# with open('dict_genders.pickle', 'wb') as handle:\n",
    "#     pickle.dump(dict_genders, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_order = pd.DataFrame(\n",
    "    df_all[df_all[\"author_genders\"].notna()].author_genders.tolist(),\n",
    "    index=df_all[df_all[\"author_genders\"].notna()].index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_order[0].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_order[1].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_order[2].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_order[3].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_order[4].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_names2(author_list):\n",
    "\n",
    "    if len(author_list) > 0:\n",
    "        names = []\n",
    "        for x in author_list:\n",
    "            words_in_name = len(x.split())\n",
    "\n",
    "            if words_in_name > 0:\n",
    "                first = re.sub(r\"[^\\w\\s]\", \"\", x.split()[0])\n",
    "            if words_in_name > 1:\n",
    "                second = re.sub(r\"[^\\w\\s]\", \"\", x.split()[1])\n",
    "            if words_in_name > 2:\n",
    "                third = re.sub(r\"[^\\w\\s]\", \"\", x.split()[2])\n",
    "            if words_in_name > 3:\n",
    "                fourth = re.sub(r\"[^\\w\\s]\", \"\", x.split()[3])\n",
    "\n",
    "            if words_in_name > 0 and len(first) > 1:\n",
    "                names.append((first, dict_genders[first]))\n",
    "            elif words_in_name > 1 and len(second) > 1:\n",
    "                names.append((second, dict_genders[second]))\n",
    "            elif words_in_name > 2 and len(third) > 1:\n",
    "                names.append((third, dict_genders[third]))\n",
    "            elif words_in_name > 3 and len(fourth) > 1:\n",
    "                names.append((fourth, dict_genders[fourth]))\n",
    "            else:\n",
    "                names.append((x, dict_genders[x]))\n",
    "\n",
    "        return names\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list1 = df_all[\"authors\"].tolist()\n",
    "node_list2 = [x for x in node_list1 if x is not None]  # remove none\n",
    "node_list3 = [item for sublist in node_list2 for item in sublist]\n",
    "node_list = list(set(node_list3))\n",
    "len(node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors = pd.DataFrame(\n",
    "    list(zip(node_list, get_first_names2(node_list))), columns=[\"full_name\", \"tuple\"]\n",
    ")\n",
    "df_authors[[\"first_name\", \"gender\"]] = pd.DataFrame(\n",
    "    df_authors.tuple.tolist(), index=df_authors.index\n",
    ")\n",
    "df_authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors[\"gender\"].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.groupby([\"eric:dateAdded\"])[\"eric:issn\"].apply(lambda x: x.isnull().mean())\n",
    "# df_all[\"dc:subject\"].iloc[328615]\n",
    "# df_all[['subjects', 'subject_top']].to_csv(\"subjects.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"eric:dateAdded\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(df_local):\n",
    "\n",
    "    node_list1 = df_local[\"authors\"].tolist()\n",
    "    node_list2 = [x for x in node_list1 if x is not None]  # remove none\n",
    "    node_list3 = [item for sublist in node_list2 for item in sublist]\n",
    "    node_list = list(set(node_list3))\n",
    "\n",
    "    n_papers_per_author = len(node_list3) / len(node_list)\n",
    "\n",
    "    edge_list1 = df_local[\"edges\"].tolist()\n",
    "    edge_list2 = [x for x in edge_list1 if x is not None]  # remove none\n",
    "    edge_list = [item for sublist in edge_list2 for item in sublist]\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(node_list)\n",
    "    G.add_edges_from(edge_list)\n",
    "    return (G, n_papers_per_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year by year (NOT cumulative)\n",
    "list1 = []\n",
    "\n",
    "for year in tqdm(range(1965, 2021)):\n",
    "\n",
    "    df_local = df_all.loc[df_all[\"eric:dateAdded\"] == str(year)]\n",
    "\n",
    "    if len(df_local) == 0:\n",
    "        continue\n",
    "\n",
    "    result = generate_graph(df_local)\n",
    "    G = result[0]\n",
    "    n_authors = len(G)\n",
    "    n_papers = len(df_local)\n",
    "\n",
    "    n_authors_per_paper = df_local[\"n_authors\"].mean()\n",
    "    n_papers_per_author = result[1]\n",
    "\n",
    "    n_collabs = nx.number_of_edges(G)\n",
    "    n_isolates = nx.number_of_isolates(G)\n",
    "    mean_collabs = 2 * G.number_of_edges() / float(G.number_of_nodes())\n",
    "\n",
    "    G_largest_comp = G.subgraph(\n",
    "        sorted(nx.connected_components(G), key=len, reverse=True)[0]\n",
    "    )\n",
    "    largest_component = len(G_largest_comp) / len(G)\n",
    "\n",
    "    deg_assort = nx.degree_assortativity_coefficient(G)\n",
    "    avg_clustering = nx.average_clustering(G)\n",
    "    transitivity = nx.transitivity(G)\n",
    "\n",
    "    list1.append(\n",
    "        (\n",
    "            year,\n",
    "            n_authors,\n",
    "            n_papers,\n",
    "            n_authors_per_paper,\n",
    "            n_papers_per_author,\n",
    "            n_collabs,\n",
    "            n_isolates,\n",
    "            mean_collabs,\n",
    "            largest_component,\n",
    "            deg_assort,\n",
    "            avg_clustering,\n",
    "            transitivity,\n",
    "        )\n",
    "    )\n",
    "\n",
    "df_summary = pd.DataFrame(\n",
    "    list1,\n",
    "    columns=[\n",
    "        \"year\",\n",
    "        \"n_authors\",\n",
    "        \"n_papers\",\n",
    "        \"n_authors_per_paper\",\n",
    "        \"n_papers_per_author\",\n",
    "        \"n_collabs\",\n",
    "        \"n_isolates\",\n",
    "        \"mean_collabs\",\n",
    "        \"largest_component\",\n",
    "        \"deg_assort\",\n",
    "        \"avg_clustering\",\n",
    "        \"transitivity\",\n",
    "    ],\n",
    ")\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"n_authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"n_papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"n_authors_per_paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"n_papers_per_author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"n_collabs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"mean_collabs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"n_isolates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"largest_component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"deg_assort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"avg_clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"transitivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "\n",
    "result = generate_graph(df_all)\n",
    "G = result[0]\n",
    "n_authors = len(G)\n",
    "n_papers = len(df_all)\n",
    "\n",
    "n_authors_per_paper = df_all[\"n_authors\"].mean()\n",
    "n_papers_per_author = result[1]\n",
    "\n",
    "n_collabs = nx.number_of_edges(G)\n",
    "n_isolates = nx.number_of_isolates(G)\n",
    "mean_collabs = 2 * G.number_of_edges() / float(G.number_of_nodes())\n",
    "\n",
    "G_largest_comp = G.subgraph(\n",
    "    sorted(nx.connected_components(G), key=len, reverse=True)[0]\n",
    ")\n",
    "largest_component = len(G_largest_comp) / len(G)\n",
    "\n",
    "deg_assort = nx.degree_assortativity_coefficient(G)\n",
    "avg_clustering = nx.average_clustering(G)\n",
    "transitivity = nx.transitivity(G)\n",
    "\n",
    "list1.append(\n",
    "    (\n",
    "        year,\n",
    "        n_authors,\n",
    "        n_papers,\n",
    "        n_authors_per_paper,\n",
    "        n_papers_per_author,\n",
    "        n_collabs,\n",
    "        n_isolates,\n",
    "        mean_collabs,\n",
    "        largest_component,\n",
    "        deg_assort,\n",
    "        avg_clustering,\n",
    "        transitivity,\n",
    "    )\n",
    ")\n",
    "\n",
    "df_overall = pd.DataFrame(\n",
    "    list1,\n",
    "    columns=[\n",
    "        \"year\",\n",
    "        \"n_authors\",\n",
    "        \"n_papers\",\n",
    "        \"n_authors_per_paper\",\n",
    "        \"n_papers_per_author\",\n",
    "        \"n_collabs\",\n",
    "        \"n_isolates\",\n",
    "        \"mean_collabs\",\n",
    "        \"largest_component\",\n",
    "        \"deg_assort\",\n",
    "        \"avg_clustering\",\n",
    "        \"transitivity\",\n",
    "    ],\n",
    ")\n",
    "df_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.info(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "\n",
    "def draw_ego(name):\n",
    "    hub_ego = nx.ego_graph(G, name)\n",
    "    pos = nx.spring_layout(hub_ego)\n",
    "    nx.draw(hub_ego, pos, node_color=\"b\", node_size=50, with_labels=True)\n",
    "    options = {\"node_size\": 300, \"node_color\": \"r\"}\n",
    "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], **options)\n",
    "\n",
    "\n",
    "draw_ego(\"Linda Darling-Hammond\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summary.to_csv(\"df_summary.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "\n",
    "for x in tqdm(G.nodes()):\n",
    "    words_in_name = len(x.split())\n",
    "\n",
    "    if words_in_name > 0:\n",
    "        first = re.sub(r\"[^\\w\\s]\", \"\", x.split()[0])\n",
    "    if words_in_name > 1:\n",
    "        second = re.sub(r\"[^\\w\\s]\", \"\", x.split()[1])\n",
    "    if words_in_name > 2:\n",
    "        third = re.sub(r\"[^\\w\\s]\", \"\", x.split()[2])\n",
    "\n",
    "    if words_in_name > 0 and len(first) > 1:\n",
    "        names.append(first)\n",
    "    elif words_in_name > 1 and len(second) > 1:\n",
    "        names.append(second)\n",
    "    elif words_in_name > 2 and len(third) > 1:\n",
    "        names.append(third)\n",
    "\n",
    "Counter(names).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.effective_size(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degs = {}\n",
    "for n in G.nodes():\n",
    "    deg = G.degree(n)\n",
    "    if deg not in degs:\n",
    "        degs[deg] = 0\n",
    "        degs[deg] += 1\n",
    "items = sorted(degs.items())\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot([k for (k, v) in items], [v for (k, v) in items])\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "plt.title(\"Wikipedia Degree Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
