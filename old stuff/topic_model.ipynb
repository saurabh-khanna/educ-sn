{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from unidecode import unidecode\n",
    "import altair as alt\n",
    "\n",
    "tqdm.pandas()\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# from genderize import Genderize\n",
    "# genderize = Genderize(\n",
    "#     user_agent='GenderizeDocs/0.0',\n",
    "#     api_key='c363eacf807f4af4992b358200ebc15c',\n",
    "#     timeout=30.0)\n",
    "\n",
    "with open(\"dict_genders.pickle\", \"rb\") as handle:\n",
    "    dict_genders = pickle.load(handle)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.extend([\"from\", \"re\", \"use\"])\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [\n",
    "        [word for word in simple_preprocess(str(doc)) if word not in stop_words]\n",
    "        for doc in texts\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_authors(dc):\n",
    "    if type(dc) == list:\n",
    "        result = [\n",
    "            unidecode(i.get(\"#text\").title().strip())\n",
    "            for i in dc\n",
    "            if i.get(\"#text\") is not None and i.get(\"@scheme\") != \"institution\"\n",
    "        ]\n",
    "        return [x for x in result if x != \"And Others\"]\n",
    "    elif dc.get(\"#text\") is not None and dc.get(\"@scheme\") != \"institution\":\n",
    "        return [unidecode(dc.get(\"#text\").title().strip())]\n",
    "\n",
    "\n",
    "def clean_name(name):\n",
    "    if \", \" in name:\n",
    "        lst = name.split(\", \")\n",
    "        lst = [item.split(\" \")[0] for item in lst]\n",
    "        return lst[1] + \" \" + lst[0]\n",
    "    elif \",\" in name:\n",
    "        lst = name.split(\",\")\n",
    "        lst = [item.split(\" \")[0] for item in lst]\n",
    "        return lst[1] + \" \" + lst[0]\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "\n",
    "def get_first_names(author_list):\n",
    "\n",
    "    if len(author_list) > 0:\n",
    "        names = []\n",
    "        for x in author_list:\n",
    "            words_in_name = len(x.split())\n",
    "\n",
    "            if words_in_name > 0:\n",
    "                first = re.sub(r\"[^\\w\\s]\", \"\", x.split()[0])\n",
    "            if words_in_name > 1:\n",
    "                second = re.sub(r\"[^\\w\\s]\", \"\", x.split()[1])\n",
    "            if words_in_name > 2:\n",
    "                third = re.sub(r\"[^\\w\\s]\", \"\", x.split()[2])\n",
    "            if words_in_name > 3:\n",
    "                fourth = re.sub(r\"[^\\w\\s]\", \"\", x.split()[3])\n",
    "\n",
    "            if words_in_name > 0 and len(first) > 1:\n",
    "                names.append(first)\n",
    "            elif words_in_name > 1 and len(second) > 1:\n",
    "                names.append(second)\n",
    "            elif words_in_name > 2 and len(third) > 1:\n",
    "                names.append(third)\n",
    "            elif words_in_name > 3 and len(fourth) > 1:\n",
    "                names.append(fourth)\n",
    "            else:\n",
    "                names.append(x)\n",
    "\n",
    "        return names\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def name_to_gender(first_name_list):\n",
    "    if first_name_list and len(first_name_list) > 0:\n",
    "        return [dict_genders[name] for name in first_name_list]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_edges(auth_list):\n",
    "    return list(combinations(auth_list, 2))\n",
    "\n",
    "\n",
    "def extract_ids(dc):\n",
    "    if type(dc) == list:\n",
    "        return [\n",
    "            i.get(\"#text\").upper().strip()\n",
    "            for i in dc\n",
    "            if i.get(\"#text\") is not None and i.get(\"@scheme\") == \"eric_accno\"\n",
    "        ][0]\n",
    "    elif dc.get(\"#text\") is not None and dc.get(\"@scheme\") == \"eric_accno\":\n",
    "        return dc.get(\"#text\").upper().strip()\n",
    "\n",
    "\n",
    "def extract_subject(dc):\n",
    "    result = []\n",
    "    for item in dc:\n",
    "        if type(item) == collections.OrderedDict:\n",
    "            result.append(unidecode(item.get(\"#text\").title().strip()))\n",
    "        elif type(item) == str:\n",
    "            result.append(unidecode(item.title().strip()))\n",
    "        else:\n",
    "            result.append(unidecode(item.title().strip()))\n",
    "    return result\n",
    "\n",
    "\n",
    "def plot_degree_dist(G):\n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    plt.hist(degrees)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = []\n",
    "\n",
    "for year in tqdm(range(1965, 2021)):\n",
    "    file_name = \"data/eric\" + str(year)\n",
    "    with open(file_name + \".xml\", encoding=\"utf-8\") as fd:\n",
    "        dict = xmltodict.parse(fd.read())\n",
    "    recs = [rec[\"metadata\"] for rec in dict[\"records\"][\"record\"]]\n",
    "    df = pd.DataFrame(recs)\n",
    "\n",
    "    df = df[df[\"dc:type\"].notna()]\n",
    "    df = df[df[\"eric:peer_reviewed\"].notna()]\n",
    "    df[\"type\"] = [\"\".join(map(str, l)).lower() for l in df[\"dc:type\"]]\n",
    "    df = df.loc[df[\"eric:peer_reviewed\"] == \"T\"]\n",
    "    # df = df[['ids', 'authors', 'edges', 'dc:type', 'dc:subject', 'eric:keywords', 'eric:keywords_geo', 'dc:title', 'eric:pageCount', 'dc:date', 'eric:dateAdded']]\n",
    "    df_all.append(df)\n",
    "\n",
    "df_all = pd.concat(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"eric:dateAdded\"] = pd.to_numeric(df_all[\"eric:dateAdded\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.loc[(df_all[\"type\"].str.contains(\"journal\"))]\n",
    "\n",
    "# get author names\n",
    "df_all[\"authors\"] = df_all.progress_apply(\n",
    "    lambda row: extract_authors(row[\"dc:creator\"]), axis=1\n",
    ")\n",
    "df_all = df_all[df_all[\"authors\"].notna()]  # remove nan authors\n",
    "df_all[\"authors\"] = df_all.progress_apply(\n",
    "    lambda row: [clean_name(item) for item in row[\"authors\"]], axis=1\n",
    ")\n",
    "\n",
    "# get edges\n",
    "df_all[\"n_authors\"] = df_all.progress_apply(lambda row: len(row[\"authors\"]), axis=1)\n",
    "df_all[\"edges\"] = df_all.progress_apply(\n",
    "    lambda row: get_edges(sorted(row[\"authors\"])), axis=1\n",
    ")\n",
    "# df_all[\"ids\"] = df_all.progress_apply(lambda row: extract_ids(row[\"dc:identifier\"]), axis=1)\n",
    "\n",
    "# get subjects\n",
    "df_all = df_all[df_all[\"dc:subject\"].notna()]\n",
    "df_all[\"subjects\"] = df_all.progress_apply(\n",
    "    lambda row: extract_subject(row[\"dc:subject\"]), axis=1\n",
    ")\n",
    "# df_all.loc[:, \"subject_top\"] = df_all.subjects.map(lambda x: x[0])\n",
    "\n",
    "# get author first name and then use it to predict gender\n",
    "df_all[\"author_first_names\"] = df_all.progress_apply(\n",
    "    lambda row: get_first_names(row[\"authors\"]), axis=1\n",
    ")\n",
    "df_all[\"author_genders\"] = df_all.progress_apply(\n",
    "    lambda row: name_to_gender(row[\"author_first_names\"]), axis=1\n",
    ")  # get genders from dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"eric:dateAdded\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "df_all[\"dc:description\"] = df_all[\"dc:description\"].map(\n",
    "    lambda x: re.sub(\"[,\\.!?]\", \"\", str(x))\n",
    ")\n",
    "# Convert to lowercase\n",
    "df_all[\"dc:description\"] = df_all[\"dc:description\"].map(lambda x: x.lower())\n",
    "# Print out the first rows of papers\n",
    "df_all[\"dc:description\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_all[\"dc:description\"].values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(\n",
    "    corpus=corpus, id2word=id2word, num_topics=num_topics\n",
    ")\n",
    "# Print the Keyword in the 10 topics\n",
    "# pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join(\"results/ldavis_prepared_\" + str(num_topics))\n",
    "\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, \"wb\") as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, \"rb\") as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(\n",
    "    LDAvis_prepared, \"results/ldavis_prepared_\" + str(num_topics) + \".html\"\n",
    ")\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(df_local):\n",
    "\n",
    "    node_list1 = df_local[\"authors\"].tolist()\n",
    "    node_list2 = [x for x in node_list1 if x is not None]  # remove none\n",
    "    node_list3 = [item for sublist in node_list2 for item in sublist]\n",
    "    node_list = list(set(node_list3))\n",
    "\n",
    "    n_papers_per_author = len(node_list3) / len(node_list)\n",
    "\n",
    "    edge_list1 = df_local[\"edges\"].tolist()\n",
    "    edge_list2 = [x for x in edge_list1 if x is not None]  # remove none\n",
    "    edge_list = [item for sublist in edge_list2 for item in sublist]\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(node_list)\n",
    "    G.add_edges_from(edge_list)\n",
    "    return (G, n_papers_per_author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUMULATIVE NOW\n",
    "list1 = []\n",
    "\n",
    "for year in tqdm(range(1965, 2021)):\n",
    "\n",
    "    df_local = df_all.loc[df_all[\"eric:dateAdded\"] <= year]\n",
    "\n",
    "    if len(df_local) == 0:\n",
    "        continue\n",
    "\n",
    "    result = generate_graph(df_local)\n",
    "    G = result[0]\n",
    "    n_authors = len(G)\n",
    "    n_papers = len(df_local)\n",
    "\n",
    "    n_authors_per_paper = df_local[\"n_authors\"].mean()\n",
    "    n_papers_per_author = result[1]\n",
    "\n",
    "    n_collabs = nx.number_of_edges(G)\n",
    "    n_isolates = nx.number_of_isolates(G)\n",
    "    mean_collabs = 2 * G.number_of_edges() / float(G.number_of_nodes())\n",
    "\n",
    "    G_largest_comp = G.subgraph(\n",
    "        sorted(nx.connected_components(G), key=len, reverse=True)[0]\n",
    "    )\n",
    "    largest_component = len(G_largest_comp) / len(G)\n",
    "\n",
    "    deg_assort = nx.degree_assortativity_coefficient(G)\n",
    "    avg_clustering = nx.average_clustering(G)\n",
    "    transitivity = nx.transitivity(G)\n",
    "\n",
    "    list1.append(\n",
    "        (\n",
    "            year,\n",
    "            n_authors,\n",
    "            n_papers,\n",
    "            n_authors_per_paper,\n",
    "            n_papers_per_author,\n",
    "            n_collabs,\n",
    "            n_isolates,\n",
    "            mean_collabs,\n",
    "            largest_component,\n",
    "            deg_assort,\n",
    "            avg_clustering,\n",
    "            transitivity,\n",
    "        )\n",
    "    )\n",
    "\n",
    "df_summary = pd.DataFrame(\n",
    "    list1,\n",
    "    columns=[\n",
    "        \"year\",\n",
    "        \"n_authors\",\n",
    "        \"n_papers\",\n",
    "        \"n_authors_per_paper\",\n",
    "        \"n_papers_per_author\",\n",
    "        \"n_collabs\",\n",
    "        \"n_isolates\",\n",
    "        \"mean_collabs\",\n",
    "        \"largest_component\",\n",
    "        \"deg_assort\",\n",
    "        \"avg_clustering\",\n",
    "        \"transitivity\",\n",
    "    ],\n",
    ")\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"n_authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"n_papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"n_authors_per_paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"n_papers_per_author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"n_collabs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"mean_collabs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"n_isolates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"largest_component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"deg_assort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"avg_clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_summary).mark_area(\n",
    "    color=\"lightblue\", interpolate=\"step-after\", line=True\n",
    ").encode(x=\"year\", y=\"transitivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "\n",
    "result = generate_graph(df_all)\n",
    "G = result[0]\n",
    "n_authors = len(G)\n",
    "n_papers = len(df_all)\n",
    "\n",
    "n_authors_per_paper = df_all[\"n_authors\"].mean()\n",
    "n_papers_per_author = result[1]\n",
    "\n",
    "n_collabs = nx.number_of_edges(G)\n",
    "n_isolates = nx.number_of_isolates(G)\n",
    "mean_collabs = 2 * G.number_of_edges() / float(G.number_of_nodes())\n",
    "\n",
    "G_largest_comp = G.subgraph(\n",
    "    sorted(nx.connected_components(G), key=len, reverse=True)[0]\n",
    ")\n",
    "largest_component = len(G_largest_comp) / len(G)\n",
    "\n",
    "deg_assort = nx.degree_assortativity_coefficient(G)\n",
    "avg_clustering = nx.average_clustering(G)\n",
    "transitivity = nx.transitivity(G)\n",
    "\n",
    "list1.append(\n",
    "    (\n",
    "        year,\n",
    "        n_authors,\n",
    "        n_papers,\n",
    "        n_authors_per_paper,\n",
    "        n_papers_per_author,\n",
    "        n_collabs,\n",
    "        n_isolates,\n",
    "        mean_collabs,\n",
    "        largest_component,\n",
    "        deg_assort,\n",
    "        avg_clustering,\n",
    "        transitivity,\n",
    "    )\n",
    ")\n",
    "\n",
    "df_overall = pd.DataFrame(\n",
    "    list1,\n",
    "    columns=[\n",
    "        \"year\",\n",
    "        \"n_authors\",\n",
    "        \"n_papers\",\n",
    "        \"n_authors_per_paper\",\n",
    "        \"n_papers_per_author\",\n",
    "        \"n_collabs\",\n",
    "        \"n_isolates\",\n",
    "        \"mean_collabs\",\n",
    "        \"largest_component\",\n",
    "        \"deg_assort\",\n",
    "        \"avg_clustering\",\n",
    "        \"transitivity\",\n",
    "    ],\n",
    ")\n",
    "df_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.info(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "\n",
    "def draw_ego(name):\n",
    "    hub_ego = nx.ego_graph(G, name)\n",
    "    pos = nx.spring_layout(hub_ego)\n",
    "    nx.draw(hub_ego, pos, node_color=\"b\", node_size=50, with_labels=True)\n",
    "    options = {\"node_size\": 300, \"node_color\": \"r\"}\n",
    "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], **options)\n",
    "\n",
    "\n",
    "draw_ego(\"Linda Darling-Hammond\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summary.to_csv(\"df_summary.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "\n",
    "for x in tqdm(G.nodes()):\n",
    "    words_in_name = len(x.split())\n",
    "\n",
    "    if words_in_name > 0:\n",
    "        first = re.sub(r\"[^\\w\\s]\", \"\", x.split()[0])\n",
    "    if words_in_name > 1:\n",
    "        second = re.sub(r\"[^\\w\\s]\", \"\", x.split()[1])\n",
    "    if words_in_name > 2:\n",
    "        third = re.sub(r\"[^\\w\\s]\", \"\", x.split()[2])\n",
    "\n",
    "    if words_in_name > 0 and len(first) > 1:\n",
    "        names.append(first)\n",
    "    elif words_in_name > 1 and len(second) > 1:\n",
    "        names.append(second)\n",
    "    elif words_in_name > 2 and len(third) > 1:\n",
    "        names.append(third)\n",
    "\n",
    "Counter(names).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.effective_size(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degs = {}\n",
    "for n in G.nodes():\n",
    "    deg = G.degree(n)\n",
    "    if deg not in degs:\n",
    "        degs[deg] = 0\n",
    "        degs[deg] += 1\n",
    "items = sorted(degs.items())\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot([k for (k, v) in items], [v for (k, v) in items])\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "plt.title(\"My Degree Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from networkx.algorithms import bipartite\n",
    "# remove = [node for node, degree in G.degree() if degree < 2]\n",
    "# G.remove_nodes_from(remove)\n",
    "# bipartite.node_redundancy(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
